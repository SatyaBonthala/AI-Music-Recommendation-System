{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "# 1. LOAD CSV AND PREPARE THE DATA\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built: 28827 nodes (471 playlists, 28356 tracks), 32833 undirected edges.\n"
     ]
    }
   ],
   "source": [
    "# Change this path to your actual CSV file.\n",
    "csv_path = \"spotify_songs.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# --- Build Unique Node Sets ---\n",
    "# Use playlists as one node set and tracks as the other.\n",
    "playlist_ids = df[\"playlist_id\"].unique()\n",
    "track_ids = df[\"track_id\"].unique()\n",
    "\n",
    "num_playlists = len(playlist_ids)\n",
    "num_tracks = len(track_ids)\n",
    "total_nodes = num_playlists + num_tracks\n",
    "\n",
    "# Create mapping dictionaries:\n",
    "# For playlists: assign indices 0 to num_playlists-1.\n",
    "playlist_id_to_index = {pid: i for i, pid in enumerate(playlist_ids)}\n",
    "# For tracks: assign indices num_playlists to total_nodes-1.\n",
    "track_id_to_index = {tid: i + num_playlists for i, tid in enumerate(track_ids)}\n",
    "\n",
    "# --- Build Graph Edges ---\n",
    "# Each row in the CSV is an occurrence of a track in a playlist.\n",
    "source_nodes = []  # playlist nodes (indices)\n",
    "target_nodes = []  # track nodes (indices)\n",
    "for _, row in df.iterrows():\n",
    "    pid = row[\"playlist_id\"]\n",
    "    tid = row[\"track_id\"]\n",
    "    if pid in playlist_id_to_index and tid in track_id_to_index:\n",
    "        source_nodes.append(playlist_id_to_index[pid])\n",
    "        target_nodes.append(track_id_to_index[tid])\n",
    "\n",
    "# For an undirected graph, add both directions.\n",
    "edge_index = torch.tensor(\n",
    "    [source_nodes + target_nodes, target_nodes + source_nodes], dtype=torch.long\n",
    ")\n",
    "\n",
    "# --- Create Node Features ---\n",
    "# We choose a set of audio feature columns for the track nodes.\n",
    "track_feature_cols = [\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"duration_ms\",\n",
    "]\n",
    "num_features = len(track_feature_cols)\n",
    "\n",
    "# Instead of initializing playlists with zeros, we initialize with random vectors.\n",
    "playlist_features = torch.randn((num_playlists, num_features), dtype=torch.float)\n",
    "\n",
    "# Build a DataFrame of unique tracks (first occurrence per track) and set its index.\n",
    "track_df = df.drop_duplicates(\"track_id\").set_index(\"track_id\")\n",
    "\n",
    "# Initialize track feature matrix.\n",
    "track_features = torch.zeros((num_tracks, num_features), dtype=torch.float)\n",
    "track_list = []  # for keeping order of track IDs\n",
    "for tid in track_ids:\n",
    "    track_list.append(tid)\n",
    "    if tid in track_df.index:\n",
    "        # Get features as floats; if conversion fails, use zeros.\n",
    "        try:\n",
    "            feats = [float(track_df.loc[tid][col]) for col in track_feature_cols]\n",
    "        except Exception as e:\n",
    "            feats = [0.0] * num_features\n",
    "        track_features[track_ids.tolist().index(tid)] = torch.tensor(\n",
    "            feats, dtype=torch.float\n",
    "        )\n",
    "    else:\n",
    "        track_features[track_ids.tolist().index(tid)] = torch.zeros(num_features)\n",
    "\n",
    "# OPTIONAL: Normalize track features (min-max scaling)\n",
    "min_vals = track_features.min(dim=0)[0]\n",
    "max_vals = track_features.max(dim=0)[0]\n",
    "# To avoid division by zero:\n",
    "range_vals = max_vals - min_vals\n",
    "range_vals[range_vals == 0] = 1.0\n",
    "track_features = (track_features - min_vals) / range_vals\n",
    "\n",
    "# Concatenate playlist and track features to create node features.\n",
    "x = torch.cat([playlist_features, track_features], dim=0)\n",
    "\n",
    "# Create the PyG Data object.\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(\n",
    "    f\"Graph built: {total_nodes} nodes ({num_playlists} playlists, {num_tracks} tracks), \"\n",
    "    f\"{edge_index.shape[1]//2} undirected edges.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "# 2. DEFINE THE GNN MODEL\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNRec(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNRec, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training, p=0.5)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "in_channels = num_features\n",
    "hidden_channels = 128\n",
    "out_channels = 64\n",
    "\n",
    "model = GNNRec(in_channels, hidden_channels, out_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "# 3. TRAIN THE GNN\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Loss: 1.3679\n",
      "Epoch 20/100 - Loss: 1.3599\n",
      "Epoch 30/100 - Loss: 1.3072\n",
      "Epoch 40/100 - Loss: 1.2534\n",
      "Epoch 50/100 - Loss: 1.2193\n",
      "Epoch 60/100 - Loss: 1.1980\n",
      "Epoch 70/100 - Loss: 1.1535\n",
      "Epoch 80/100 - Loss: 1.1341\n",
      "Epoch 90/100 - Loss: 1.0899\n",
      "Epoch 100/100 - Loss: 1.0620\n"
     ]
    }
   ],
   "source": [
    "# Prepare positive edges (only one direction is used for training)\n",
    "pos_playlist_nodes = torch.tensor(source_nodes, dtype=torch.long)\n",
    "pos_track_nodes = torch.tensor(target_nodes, dtype=torch.long)\n",
    "num_pos_edges = pos_playlist_nodes.shape[0]\n",
    "\n",
    "num_epochs = 10000\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "    # Compute positive scores (dot product between playlist and track embeddings)\n",
    "    pos_playlist_emb = embeddings[pos_playlist_nodes]\n",
    "    pos_track_emb = embeddings[pos_track_nodes]\n",
    "    pos_scores = (pos_playlist_emb * pos_track_emb).sum(dim=1)\n",
    "    pos_loss = -torch.log(torch.sigmoid(pos_scores) + 1e-15).mean()\n",
    "\n",
    "    # Negative sampling: sample random playlist-track pairs.\n",
    "    neg_playlist_nodes = torch.randint(0, num_playlists, (num_pos_edges,))\n",
    "    neg_track_nodes = torch.randint(num_playlists, total_nodes, (num_pos_edges,))\n",
    "    neg_playlist_emb = embeddings[neg_playlist_nodes]\n",
    "    neg_track_emb = embeddings[neg_track_nodes]\n",
    "    neg_scores = (neg_playlist_emb * neg_track_emb).sum(dim=1)\n",
    "    neg_loss = -torch.log(1 - torch.sigmoid(neg_scores) + 1e-15).mean()\n",
    "\n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "# 4. INTERACTIVE RECOMMENDATION\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Interactive Song Recommendation ---\n",
      "Type a song name (exactly as in the dataset) to get 10 similar song recommendations.\n",
      "Type 'quit' to exit.\n",
      "\n",
      "\n",
      "Top 10 recommended songs similar to 'closer':\n",
      "1. Hold On\n",
      "2. Bleed\n",
      "3. Only You\n",
      "4. Bad Day\n",
      "5. The Rest Will Follow\n",
      "6. Dream On\n",
      "7. I Don't Know\n",
      "8. Something New (feat. Allison Graaff)\n",
      "9. Not Ok\n",
      "10. So It Goes\n",
      "\n",
      "\n",
      "Exiting recommendation system.\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode and compute final embeddings.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "# Separate playlist and track embeddings.\n",
    "playlist_embeddings = final_embeddings[:num_playlists]  # indices 0 .. num_playlists-1\n",
    "track_embeddings = final_embeddings[\n",
    "    num_playlists:\n",
    "]  # indices num_playlists .. total_nodes-1\n",
    "\n",
    "# Build an inverse mapping from track node index (global index) to track_id.\n",
    "inv_track_map = {v: k for k, v in track_id_to_index.items()}\n",
    "\n",
    "\n",
    "def get_track_node_index_by_name(query_name):\n",
    "    \"\"\"\n",
    "    Given a song name, return (track_id, global_node_index) if found.\n",
    "    The search is case-insensitive and expects an exact match.\n",
    "    \"\"\"\n",
    "    matches = track_df[track_df[\"track_name\"].str.lower() == query_name.lower()]\n",
    "    if matches.empty:\n",
    "        return None, None\n",
    "    else:\n",
    "        track_id = matches.index[0]\n",
    "        node_index = track_id_to_index.get(track_id)\n",
    "        return track_id, node_index\n",
    "\n",
    "\n",
    "print(\"\\n--- Interactive Song Recommendation ---\")\n",
    "print(\n",
    "    \"Type a song name (exactly as in the dataset) to get 10 similar song recommendations.\"\n",
    ")\n",
    "print(\"Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    query_name = input(\"Enter a song name: \").strip()\n",
    "    if query_name.lower() == \"quit\":\n",
    "        print(\"Exiting recommendation system.\")\n",
    "        break\n",
    "\n",
    "    track_id, node_index = get_track_node_index_by_name(query_name)\n",
    "    if node_index is None:\n",
    "        print(\"Song not found. Please try again.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Retrieve the embedding of the query song.\n",
    "    song_embedding = final_embeddings[node_index]\n",
    "\n",
    "    # Compute cosine similarities between the query song and all track embeddings.\n",
    "    # (Using cosine similarity can help mitigate scale differences.)\n",
    "    # Normalize embeddings for cosine similarity.\n",
    "    song_norm = song_embedding / song_embedding.norm(p=2)\n",
    "    track_norms = F.normalize(track_embeddings, p=2, dim=1)\n",
    "    similarities = torch.matmul(track_norms, song_norm)\n",
    "\n",
    "    # Exclude the query song from recommendations.\n",
    "    local_index = node_index - num_playlists\n",
    "    if 0 <= local_index < similarities.shape[0]:\n",
    "        similarities[local_index] = -float(\"inf\")\n",
    "\n",
    "    # Get the top-10 recommended track indices (local indices in track_embeddings).\n",
    "    topk = 10\n",
    "    top_sim_values, top_indices = torch.topk(similarities, topk)\n",
    "\n",
    "    recommended_track_names = []\n",
    "    for local_idx in top_indices.tolist():\n",
    "        global_node_idx = (\n",
    "            local_idx + num_playlists\n",
    "        )  # convert local index to global index\n",
    "        rec_track_id = inv_track_map.get(global_node_idx, None)\n",
    "        if rec_track_id is not None:\n",
    "            rec_track_name = track_df.loc[rec_track_id][\"track_name\"]\n",
    "            recommended_track_names.append(rec_track_name)\n",
    "        else:\n",
    "            recommended_track_names.append(\"Unknown Track\")\n",
    "\n",
    "    print(f\"\\nTop 10 recommended songs similar to '{query_name}':\")\n",
    "    for i, name in enumerate(recommended_track_names, 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
